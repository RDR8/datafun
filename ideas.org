* thoughts on polymorphism and type inference

currently the implementation has subtyping, but the only non-trivial subtyping
is (a ~> b) <: (a -> b).

however, we have two forms of type quantification:
- quantification over all types
- qunatification over lattice types

using Hindley-Milner unmodified probably won't work, because how can we
determine whether a function is intended to be ordinary or monotone?

also, have to incorporate typeclass-like constraint for quantification over
lattice types.

* implementation: maybe use PLT redex?
* Future work: generalize to {some,many,all} Eilenberg-Moore categories?
* Future work: exploring lists, bags, and sets
counting things, for example, is naturally a commutative-monoid-style
computation.

* Future work: a type system for LVars?
* Future work: on-line monotone computation

Matthew Maurer is working on a datalog-like logic language for binary analysis
(https://github.com/maurer/holmes/blob/master/formal/holmes.tex) which has the
property that external predicates (representing individual analyses) can have
new facts added at any time! This requires monotonicity in aggregation
operators.

We already have monotonicity! Can we efficiently implement *on-line*
computation? That is to say, for a function (f : A ~> B) in our language, is
there an efficient way to evaluate it over *monotonically increasing* inputs?

Could use SAC (self-adjusting computation), but that's more general (works for
*arbitrarily-changing* inputs). (A monotonic function can of course involve
computing things in the non-monotonic fragment, but I *think* we're guaranteed
those won't change when we change the monotone input?)
